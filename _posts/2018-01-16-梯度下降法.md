---
layout: post
title:      "梯度下降法"
subtitle:   "--机器学习算法"
date:       2018-01-16
author:     "任庭玉"
catalog: true
tags:
    - 机器学习
    - 算法
comments: true
excerpt: 梯度下降法的作用：我们在进行算法的模型参数调优时，其实就是要做到损失函数最小化，梯度下降就是要解决损失函数最小化的问题。求解同类问题的方法还有最小二乘法。讲解梯度下降法需要了解以下概念...
imgPath: "#"
---

# 梯度下降法
梯度下降法的作用：我们在进行算法的模型参数调优时，其实就是要做到损失函数最小化，梯度下降就是要解决损失函数最小化的问题。求解同类问题的方法还有最小二乘法。

讲解梯度下降法需要了解以下概念：

 1. 梯度：在高等数学的微积分中，我们学习过偏导数$\partial$的概念。f(x,y)对x,y求偏导数分别得$\partial$f(x,y)/$\partial$x, $\partial$f(x,y)/$\partial$y，而梯度就是各个参数偏导数的向量表达形式。比如向量($\partial$f(x,y)/$\partial$x, $\partial$f(x,y)/$\partial$y)就是f(x,y)的梯度。而($\partial$f($x_{0}$,$y_{0}$)/$\partial x_{0}$, $\partial$f($x_{0}$,$y_{0}$)/$\partial y_{0}$)就是函数f(x,y)在($x_{0}$,$y_{0}$)的梯度向量。梯度的几何意义是函数变化最快的方向，所以沿着梯度的方向我们能够很快寻找到函数最大值，沿着负梯度方向我们能够很快寻找到函数最小值。
 2. 步长(learning rate)：梯度下降法是一个迭代方法，每一次迭代，都在向损失函数的最小值靠近一步。步长就是在一次迭代过程中向梯度副方向前进的长度。
 3. 特征(feature)：假设损失函数为f($x_{0},x_{1},x_{2},x_{3}$)，则$x_{0},x_{1},x_{2},x_{3}$就是特征。
 4. 拟合函数：算法中为了拟合样本而使用的假设函数。
 5. 损失函数：用于参数调优，主要用来评估算法的好坏。一般损失函数的定义是样本输出与拟合函数的差的平方和。

梯度下降法的流程：

 1. 假设对于线性回归，样本数为n (格式为($x_{0}^{i} ,x_{1}^{i} ,x_{2} ^{i},x_{3}^{i} ,y_{i}$)) i$\in$[0,n]，拟合函数为f($x_{0},x_{1},x_{2},x_{3}$) = $\theta_{0} x_{0}$ + $\theta_{1} x_{1}$ + $\theta_{2} x_{2}$ + $\theta_{3} x_{3}$，损失函数为 h($\theta_{0},\theta_{1},\theta_{2},\theta_{3}$) = 1/2n $\sum_{i=0}^n$ $(f_{i}(x_{0},x_{1},x_{2},x_{3}) - y_{i})^{2}$
 2. 初始化参数$\theta_{0},\theta_{1},\theta_{2},\theta_{3}$，算法终止距离end，步长m，一般初始化所有参数$\theta_{i}$为0，步长为1
 3. 计算当前位置损失函数的梯度 $\partial h(\theta_{0},\theta_{1},\theta_{2},\theta_{3})/ \partial\theta_{i}$
 4. 步长乘以当前损失函数梯度 $m\partial h(\theta_{0},\theta_{1},\theta_{2},\theta_{3})/ \partial\theta_{i}$
 5. 确定是否所有的$\theta_{i}$梯度下降的距离都小于end，若果是迭代结束，如果不是进行步骤6
 6. 更新$\theta_{i}$，公式为 $\theta_{i}$ = $\theta_{i}$ - $m\partial h(\theta_{0},\theta_{1},\theta_{2},\theta_{3})/ \partial\theta_{i}$，回到步骤3


