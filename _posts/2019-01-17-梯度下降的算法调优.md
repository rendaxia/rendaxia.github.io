---
layout: post
title:      "梯度下降的算法调优"
subtitle:   "--机器学习算法"
date:       2019-01-17
author:     "任庭玉"
catalog: true
tags:
    - 机器学习
    - 算法
comments: true
excerpt: 梯度下降法的作用：我们在进行算法的模型参数调优时，其实就是要做到损失函数最小化，梯度下降就是要解决损失函数最小化的问题。求解同类问题的方法还有最小二乘法。讲解梯度下降法需要了解以下概念...
imgPath: "#"
---

# 梯度下降调优

前一章讲述了梯度下降的基本内容，本章讲述的是梯度下降的算法调优，调优的方面在以下三点。

 1. 步长调整。步长是在每次迭代时负梯度下降的距离，所以步长太大，容易造成错过最优解的情况，步长太小，迭代次数过多，算法执行时间过长。为了折中选择，可以将步长按照从大到小的顺序开始测试，知道调整到损失函数达到最小值为止。
 2. 参数初始值选择。前面一章所讲的$\theta_{i}$的选择一般默认是0，但是梯度下降这个方法本身就是一种求解局部最优解（下面会讲到）的方法，如果初始值选择只有一种，很容易造成局部解的情况，所以要选择多组初始值进行测试。
 3. 归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢（迭代结束的判断是每个特征的参数的单次迭代距离都要小于最终距离end），为了减少特征取值的影响，可以对特征数据归一化，是迭代速度加快。

> 归一化：对于样本的每一个特征的取值$x_{i}$，先求得$x_{i}$的期望$\bar{x_{i}}$和标准差std($x_{i}$)，使用公式$x_{i}new = x_{i} - \bar{x_{i}} / std(x_{i})$即可

> 局部最优：局部最优解是在某个范围区间内达到了最优值，但是在整个自变量区间内并不是最优值。假设对于单特征函数，在凸函数的情况下必然能取到全局最小值，但是在类似于多个凹凸函数相连的函数下，梯度下降所取到的值是局部最小值。

参考博文：[梯度下降（Gradient Descent）小结][1]


[1]: https://www.cnblogs.com/pinard/p/5970503.html