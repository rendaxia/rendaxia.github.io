---
layout: post
title:      "拉普拉斯平滑最简单理解"
subtitle:   "--贝叶斯、机器学习"
date:       2018-11-16
author:     "任庭玉"
catalog: true
tags:
    - 机器学习
    - 文本分析
comments: true
excerpt: 拉普拉斯平滑解决问题及最简单理解...
imgPath: "https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1542356968078&di=935fbea9148363614cd7e9b46cdf4bcf&imgtype=0&src=http://5b0988e595225.cdn.sohucs.com/images/20180216/686bc5b48c394fca88f65e26cc2eeb8b.jpeg"
---

# 拉普拉斯平滑最简单理解
![此处输入图片的描述][1]
我们在遇到分类数据集的问题时，某一条数据不存在某一个特征

比如遇到垃圾评论分类时，这条数据分词后的某个词汇并没有出现在垃圾评论中，所以在计算概率的情况下
p1 = p(该词汇|垃圾) = 该词汇在垃圾评论中出现次数 / 垃圾评论出现单词总数 = 0
所以该词汇出现的情况下，该评论为垃圾评论的概率为
p(垃圾|该词汇)= p1*p(垃圾)/p(该词汇) = 0

而该评论是否为垃圾评论的概率判别公式为
p = p(垃圾|词汇1)*...*p(垃圾|词汇n)/[p(垃圾|词汇1)*...p(垃圾|词汇n)+(1-p(垃圾|词汇1))*...(1-p(垃圾|词汇n))]
由此公式明显看出，一个单词的概率为0会导致整个评论的概率为0

这是不准确的，所以为了避免该情况的发生，贝叶斯引入了拉普拉斯平滑，理解起来很简单：
p(该词汇|垃圾) = 该词汇在垃圾评论中出现次数 / 垃圾评论出现单词总数
改为
p(该词汇|垃圾) = 该词汇在垃圾评论中出现次数+1 / 垃圾评论出现单词总数+分类类别数目（比如当前例子就是为了区分正常评论和垃圾评论，所以类别数目为2）
这样就很好的避免了概率为0的情况


[1]: https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1542356968078&di=935fbea9148363614cd7e9b46cdf4bcf&imgtype=0&src=http://5b0988e595225.cdn.sohucs.com/images/20180216/686bc5b48c394fca88f65e26cc2eeb8b.jpeg