---
layout: post
title:      "信息论基础知识总结"
subtitle:   "--机器学习理论"
date:       2018-11-21
author:     "任庭玉"
catalog: true
tags:
    - 机器学习
    - 信息论
comments: true
excerpt: 信息论是应用数学的一个分支，主要解决了对一个信号能够提供信息的多少进行量化的问题，最初用于研究在一个含有噪声的信道上用离散的字母表来发送消息，指导最优的通信编码等，而在机器学习中经常需要使用它们的关键思想来描述概率分布或者量化概率分布之间的相似性...
imgPath: "https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1542819594760&di=ffb13097373cc1e7b109cc83bd549db1&imgtype=0&src=http://c.hiphotos.baidu.com/zhidao/wh=450,600/sign=8f876147b9fb43161a4a727e15946a15/72f082025aafa40ff911cf51a064034f79f01962.jpg"
---

# 信息论基础知识总结
![香农信息论][1]

之前看了一篇很好的博文[《成为一名机器学习算法工程师，你需要这些必备技能》][2]，讲述了我们成为一个合格的算法工程师，需要学习和掌握的内容，其中包含了信息论的基础知识，特此做一个总结。

1.什么是信息论，解决了什么问题？
信息论是应用数学的一个分支，主要解决了对一个信号能够提供信息的多少进行量化的问题，最初用于研究在一个含有噪声的信道上用离散的字母表来发送消息，指导最优的通信编码等，而在机器学习中经常需要使用它们的关键思想来描述概率分布或者量化概率分布之间的相似性。

2.自信息
自信息是与与概率空间中的单一事件或离散随机变量的值相关的信息量的量度。一个随机产生的事件所包含的自信息数量，只与事件发生的概率相关。事件发生的概率越低，在事件真的发生时，接收到的信息中，包含的自信息越大。此外，自信息的是正的而且是可加的。如果事件 C 是两个独立事件 A 和 B 的交集，那么宣告 C 发生的信息量就等于分别宣告事件 A 和事件 B 的信息量的和：I(A ∩ B)=I(A)+I(B).
> 自信息计算公式：I(x) = −logP(x) = logP(1/x)

3.信息熵
随机事件x的自信息期望值，实质上就是求各个概率下的自信息值与其概率和的乘积。公式为：
> H(X)= [p(x1)logP(x1)+ ... +p(xn)logP(xn)]

决策树算法中就使用信息熵的概念选择信息增益最大的特征属性作为根节点。

4.条件熵
条件熵描述了在已知第二个随机变量X的值的前提下，随机变量Y的信息熵还有多少。基于条件X 的Y的信息熵，用H（Y|X) 表示。

5.互信息
(1)描述两个随机变量的关联程度，关联程度为0（即两个变量相互独立）则互信息为0公式为：

> I(x;y) = ∑x ∑y [p(x,y)log(p(x,y)/p(x)p(y))]

也可以把互信息看成由于知道y值而造成的x的不确定性的减小(反之亦然)（即Y的值透露了多少关于X 的信息量），比如如下公式：
> I(x,y) = H(x) - H(x|y) = H(y) - H(y|x)

所以，如果X,Y关系越密切，I(X,Y)就越大。I(X,Y)最大的取值是H(Y)，此时H(Y|X)为0，意义为X和Y完全相关，在X确定的情况下Y是个定值，没有出现其他不确定情况的概率，所以为H(Y|X)为0
互信息的应用：（1）文本自动摘要（2）两个词语同时出现的概率（3）特征提取时特征项与类别的关联程度 

6.点互信息
点互信息实质上就是x,y两个随机变量取特定值时两个定量的相关性。所以互信息实质上就是点互信息的加权和

7.相对熵
又称互熵，鉴别信息，Kullback熵，Kullback-Leible散度（KL散度）等。
设p(x)和q(x)是x取值的两个概率分布，典型情况下，P表示数据的真实分布，Q表示数据的理论分布或P的近似分布。则P对Q的相对熵为：

> D(p||q) = p(x1)log[p(x1)/q(x1)]+ ... + p(xn)log[p(xn)/q(xn)]

相对熵的应用：相对熵是比较两个概率分布的距离（相似度），因此可以用于文本相似度的计算；还可以用于权重指标的分配。

8.交叉熵
针对上一个相对熵的公式，我们可以做如下变形：
> D(p||q) = p(x1)log[p(x1)/q(x1)]+ ... + p(xn)log[p(xn)/q(xn)] = p(x1)log[p(x1)]+ ... + p(xn)log[p(xn)] - p(x1)log[q(x1)] - ... -p(xn)log[q(xn)] = -p的熵 - [p(x1)log[q(x1)]+ ... +p(xn)log[q(xn)]

上述公式中p(x1)log[q(x1)]+ ... +p(xn)log[q(xn)就是我们的交叉熵，因为p的熵不会变化，所以我们一般只考虑交叉熵这个变化的部分。


参考博文：
[互信息（Mutual Information）的介绍][3]
[机器学习信息论基础][4]
[一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉][5]


[1]: https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1542819594760&di=ffb13097373cc1e7b109cc83bd549db1&imgtype=0&src=http://c.hiphotos.baidu.com/zhidao/wh=450,600/sign=8f876147b9fb43161a4a727e15946a15/72f082025aafa40ff911cf51a064034f79f01962.jpg
[2]: https://blog.csdn.net/qq_40027052/article/details/78773302
[3]: https://blog.csdn.net/qq_15111861/article/details/80724278
[4]: https://blog.csdn.net/qq_39388410/article/details/79128392
[5]: https://blog.csdn.net/tsyccnh/article/details/79163834